---
title: "Amicus Word Count"
author: "Amanda Konet"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, error = F)

# Box access
library(boxr)
box_auth()

# data manip
library(tidyverse)
library(tidytext)
```

# Purpose

Get word count of amicus briefs -- min, max, med, avg. May need to use long transformers instead of regular since we have some pretty long docs?

consult w/ JSS

```{r}
# shortened amicus text
amicus <- box_read("880862059511")

amicus %>% head(5)
```

# Tokenize

Tokenize text as is then get # words per brief

```{r}
# returns case, brief, word for all words in all cases
amicus_tokens <- amicus %>% 
  select(case, brief, text = txt_short) %>%
  group_by(brief) %>% 
  unnest_tokens(word, text)

# word ct per brief
amicus_wrdct <- amicus_tokens %>% group_by(brief) %>% summarize(n=n())
```


# Explore word count

Summary - way higher than BERT. 

```{r}
summary(amicus_wrdct$n)
```


Number of docs w/ > 512 tokens (regular transformers), > 4,096 tokens (longformer), > 16k tokens (longformer w/adjustments & computing power)

```{r}
amicus_wrdct <- amicus_wrdct %>% 
  mutate(greater_512 = n > 512,
         num_blocks_512 = ceiling(n/512),
         greater_4096 = n > 4096,
         num_blocks_4096 = ceiling(n/4096),
         greater_16k = n > 16000,
         num_blocks_16k = ceiling(n/16000))

# num docs at each length
amicus_wrdct %>% 
  summarize(greater_512 = sum(greater_512*1),
            greater_4096 = sum(greater_4096*1),
            greater_16k = sum(greater_16k*1))
```

How many processing blocks would we need for each model?

```{r}
amicus_wrdct %>% 
  filter(brief != "Ayotte v Planned Parenthood of Northern New England. Amici Brief for Petitioner%2c by Loren Leman et al.docx") %>% 
  summarize(med_blocks_512 = median(num_blocks_512),
            med_blocks_4096 = median(num_blocks_4096),
            med_blocks_16k = median(num_blocks_16k))
```


LongFormers can use up to 16k tokens... any texts that are longer than this? Yes. We have 45 amicus briefs with 16k+ tokens. Possible we may be able to pare these down but it will have to be manual and we need to make sure we aren't losing information. 


```{r}
amicus_wrdct %>% filter(n >= 16000) %>% arrange(-n)
```

